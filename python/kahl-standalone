#!/usr/bin/env python3
"""Standalone secrets-filter with patterns baked in.

Generated by generate.py - DO NOT EDIT
Source hash: 97057734274b
Generated: 2026-01-10T07:13:13Z

This is a standalone version of secrets-filter with all pattern definitions
embedded. Deploy to ~/.claude/secrets-filter for use with Claude Code.

To regenerate: python python/generate.py
"""
import math
import os
import re
import sys
from collections import Counter


# ============================================================================
# Pattern Data (generated from YAML)
# ============================================================================

# Constants
LONG_THRESHOLD = 50
MAX_PRIVATE_KEY_BUFFER = 100

# Direct patterns: (regex_string, label)
PATTERNS = [
    (r'ghp_[A-Za-z0-9]{36}', 'GITHUB_PAT'),
    (r'gho_[A-Za-z0-9]{36}', 'GITHUB_OAUTH'),
    (r'ghs_[A-Za-z0-9]{36}', 'GITHUB_SERVER'),
    (r'ghr_[A-Za-z0-9]{36}', 'GITHUB_REFRESH'),
    (r'github_pat_[A-Za-z0-9_]{22,}', 'GITHUB_PAT'),
    (r'glpat-[A-Za-z0-9_-]{20,}', 'GITLAB_PAT'),
    (r'xoxb-[0-9]+-[0-9A-Za-z-]+', 'SLACK_BOT'),
    (r'xoxp-[0-9]+-[0-9A-Za-z-]+', 'SLACK_USER'),
    (r'xoxa-[0-9]+-[0-9A-Za-z-]+', 'SLACK_APP'),
    (r'xoxs-[0-9]+-[0-9A-Za-z-]+', 'SLACK_SESSION'),
    (r'sk-[A-Za-z0-9]{48}', 'OPENAI_KEY'),
    (r'sk-proj-[A-Za-z0-9_-]{20,}', 'OPENAI_PROJECT_KEY'),
    (r'sk-ant-[A-Za-z0-9-]{90,}', 'ANTHROPIC_KEY'),
    (r'AKIA[A-Z0-9]{16}', 'AWS_ACCESS_KEY'),
    (r'AIza[A-Za-z0-9_-]{35}', 'GOOGLE_API_KEY'),
    (r'AGE-SECRET-KEY-[A-Z0-9]{59}', 'AGE_SECRET_KEY'),
    (r'sk_live_[A-Za-z0-9]{24,}', 'STRIPE_SECRET'),
    (r'sk_test_[A-Za-z0-9]{24,}', 'STRIPE_TEST'),
    (r'pk_live_[A-Za-z0-9]{24,}', 'STRIPE_PUBLISHABLE'),
    (r'SK[a-f0-9]{32}', 'TWILIO_KEY'),
    (r'SG\.[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+', 'SENDGRID_KEY'),
    (r'npm_[A-Za-z0-9]{36}', 'NPM_TOKEN'),
    (r'pypi-[A-Za-z0-9_-]{100,}', 'PYPI_TOKEN'),
    (r'eyJ[A-Za-z0-9_-]{10,}\.[A-Za-z0-9_-]+\.[A-Za-z0-9_-]+', 'JWT_TOKEN'),
    (r'dop_v1_[a-f0-9]{64}', 'DIGITALOCEAN_PAT'),
    (r'doo_v1_[a-f0-9]{64}', 'DIGITALOCEAN_OAUTH'),
    (r'dor_v1_[a-f0-9]{64}', 'DIGITALOCEAN_REFRESH'),
    (r'HRKU-[A-Za-z0-9_]{60}', 'HEROKU_API_KEY'),
    (r'GOCSPX-[A-Za-z0-9_-]{20,}', 'GCP_OAUTH_SECRET'),
    (r'nfp_[A-Za-z0-9_-]{20,}', 'NETLIFY_PAT'),
    (r'sb_publishable_[A-Za-z0-9_-]{20,}', 'SUPABASE_PUBLISHABLE'),
    (r'sb_secret_[A-Za-z0-9_-]{20,}', 'SUPABASE_SECRET'),
    (r'pscale_tkn_[A-Za-z0-9_-]{30,}', 'PLANETSCALE_TOKEN'),
    (r'[A-Za-z0-9]{9}\.atlasv1\.[A-Za-z0-9]{40,}', 'TERRAFORM_CLOUD_TOKEN'),
    (r'hvs\.[A-Za-z0-9]{24,}', 'VAULT_SERVICE_TOKEN'),
    (r'hvb\.[A-Za-z0-9]{24,}', 'VAULT_BATCH_TOKEN'),
    (r'hvr\.[A-Za-z0-9]{24,}', 'VAULT_RECOVERY_TOKEN'),
    (r'rubygems_[a-f0-9]{56}', 'RUBYGEMS_API_KEY'),
    (r'shpat_[A-Za-z0-9]{32,}', 'SHOPIFY_ADMIN_TOKEN'),
    (r'NRAK-[A-Za-z0-9]{27,}', 'NEWRELIC_USER_KEY'),
    (r'key-[A-Za-z0-9]{32,}', 'MAILGUN_API_KEY'),
    (r'lin_api_[A-Za-z0-9]{40,}', 'LINEAR_API_KEY'),
    (r'dp\.pt\.[A-Za-z0-9]{40,}', 'DOPPLER_TOKEN'),
    (r'[0-9]{9,10}:[A-Za-z0-9_-]{35,}', 'TELEGRAM_BOT_TOKEN'),
    (r'[MN][A-Za-z0-9]{23,}\.[A-Za-z0-9_-]{6}\.[A-Za-z0-9_-]{27,}', 'DISCORD_BOT_TOKEN'),
    (r'https://discord\.com/api/webhooks/[0-9]+/[A-Za-z0-9_-]+', 'DISCORD_WEBHOOK'),
    (r'EAA[A-Za-z0-9]{50,}', 'FACEBOOK_ACCESS_TOKEN'),
]

# Context patterns using lookbehind: (regex_string, label)
CONTEXT_PATTERNS = [
    (r'(?<=password )[^\s]+', 'NETRC_PASSWORD'),
    (r'(?<=passwd )[^\s]+', 'NETRC_PASSWORD'),
    ('(?<=password=)[^\\s,;"\'\\}\\[\\]]+', 'PASSWORD_VALUE'),
    ('(?<=password:)\\s*[^\\s,;"\'\\}\\[\\]]+', 'PASSWORD_VALUE'),
    ('(?<=secret=)[^\\s,;"\'\\}\\[\\]]+', 'SECRET_VALUE'),
    ('(?<=secret:)\\s*[^\\s,;"\'\\}\\[\\]]+', 'SECRET_VALUE'),
    ('(?<=token=)[^\\s,;"\'\\}\\[\\]]+', 'TOKEN_VALUE'),
    ('(?<=token:)\\s*[^\\s,;"\'\\}\\[\\]]+', 'TOKEN_VALUE'),
    (r'(?<=AccountKey=)[A-Za-z0-9+/]{88}==', 'AZURE_STORAGE_KEY'),
    ('(?<=Password=)[^\\s,;"\'\\}\\[\\]]+', 'PASSWORD_VALUE'),
    ('(?<=Password:)\\s*[^\\s,;"\'\\}\\[\\]]+', 'PASSWORD_VALUE'),
    ('(?<=Secret=)[^\\s,;"\'\\}\\[\\]]+', 'SECRET_VALUE'),
    ('(?<=Secret:)\\s*[^\\s,;"\'\\}\\[\\]]+', 'SECRET_VALUE'),
    ('(?<=Token=)[^\\s,;"\'\\}\\[\\]]+', 'TOKEN_VALUE'),
    ('(?<=Token:)\\s*[^\\s,;"\'\\}\\[\\]]+', 'TOKEN_VALUE'),
]

# Special patterns with capture groups
SPECIAL_PATTERNS = {
    'git_credential': {
        'pattern': r'(://[^:]+:)([^@]+)(@)',
        'label': 'GIT_CREDENTIAL',
        'secret_group': 2,
    },
    'docker_auth': {
        'pattern': r'("auth":\s*")([A-Za-z0-9+/=]{20,})(")',
        'label': 'DOCKER_AUTH',
        'secret_group': 2,
    },
}

# Private key markers for streaming state machine
PRIVATE_KEY_BEGIN = r'-----BEGIN [A-Z ]*PRIVATE KEY-----'
PRIVATE_KEY_END = r'-----END [A-Z ]*PRIVATE KEY-----'

# Environment variable detection
EXPLICIT_ENV_VARS = {
    'ALGOLIA_ADMIN_KEY',
    'ALGOLIA_API_KEY',
    'ANTHROPIC_API_KEY',
    'AWS_SECRET_ACCESS_KEY',
    'AWS_SESSION_TOKEN',
    'AZURE_CLIENT_SECRET',
    'BITBUCKET_TOKEN',
    'CLAUDE_API_KEY',
    'CLOUDFLARE_API_KEY',
    'CLOUDFLARE_API_TOKEN',
    'CONSUL_HTTP_TOKEN',
    'DATABASE_URL',
    'DATADOG_API_KEY',
    'DATADOG_APP_KEY',
    'DIGITALOCEAN_ACCESS_TOKEN',
    'DIGITALOCEAN_TOKEN',
    'DISCORD_BOT_TOKEN',
    'DISCORD_TOKEN',
    'DOCKER_PASSWORD',
    'DOPPLER_TOKEN',
    'ENCRYPTION_KEY',
    'FACEBOOK_ACCESS_TOKEN',
    'GH_TOKEN',
    'GITHUB_TOKEN',
    'GITLAB_TOKEN',
    'GLAB_TOKEN',
    'GRAFANA_API_KEY',
    'HEROKU_API_KEY',
    'HEROKU_AUTH_TOKEN',
    'JWT_SECRET',
    'LINEAR_API_KEY',
    'MAILGUN_API_KEY',
    'MONGODB_URI',
    'NEON_API_KEY',
    'NETLIFY_AUTH_TOKEN',
    'NEWRELIC_API_KEY',
    'NEW_RELIC_LICENSE_KEY',
    'NPM_TOKEN',
    'OPENAI_API_KEY',
    'PLANETSCALE_SERVICE_TOKEN',
    'PYPI_TOKEN',
    'REDIS_URL',
    'SENDGRID_API_KEY',
    'SENTRY_AUTH_TOKEN',
    'SENTRY_DSN',
    'SESSION_SECRET',
    'SHOPIFY_ACCESS_TOKEN',
    'SHOPIFY_API_KEY',
    'SHOPIFY_API_SECRET',
    'SLACK_BOT_TOKEN',
    'SLACK_TOKEN',
    'SLACK_WEBHOOK_URL',
    'STRIPE_SECRET_KEY',
    'SUPABASE_ANON_KEY',
    'SUPABASE_SERVICE_ROLE_KEY',
    'TELEGRAM_BOT_TOKEN',
    'TERRAFORM_CLOUD_TOKEN',
    'TFC_TOKEN',
    'TF_TOKEN',
    'TWILIO_AUTH_TOKEN',
    'VAULT_TOKEN',
    'VERCEL_TOKEN',
}

ENV_SUFFIXES = (
    '_SECRET',
    '_PASSWORD',
    '_TOKEN',
    '_API_KEY',
    '_PRIVATE_KEY',
    '_AUTH',
    '_CREDENTIAL',
)

# Entropy detection configuration
ENTROPY_ENABLED_DEFAULT = False

ENTROPY_THRESHOLDS = {
    'hex': 3.0,
    'base64': 4.5,
    'alphanumeric': 4.5,
}

ENTROPY_MIN_LENGTH = 16
ENTROPY_MAX_LENGTH = 256

ENTROPY_EXCLUSIONS = [
    {
        'pattern': r'[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}',
        'label': 'UUID',
        'case_insensitive': True,
        'context_keywords': None,
    },
    {
        'pattern': r'[a-f0-9]{40}',
        'label': 'GIT_SHA',
        'case_insensitive': False,
        'context_keywords': ['commit', 'sha', 'hash', 'ref', 'oid', 'blob', 'tree', 'parent', 'HEAD', 'merge'],
    },
    {
        'pattern': r'[a-f0-9]{64}',
        'label': 'SHA256_HASH',
        'case_insensitive': False,
        'context_keywords': ['sha256', 'image', 'docker', 'digest', 'hash', 'checksum', 'layer'],
    },
    {
        'pattern': r'[a-f0-9]{32}',
        'label': 'MD5_HASH',
        'case_insensitive': False,
        'context_keywords': ['md5', 'hash', 'checksum', 'etag'],
    },
    {
        'pattern': r'sha256:[a-f0-9]{64}',
        'label': 'DOCKER_DIGEST',
        'case_insensitive': False,
        'context_keywords': None,
    },
    {
        'pattern': r'[0-9]+\.[0-9]+\.[0-9]+[a-f0-9]*',
        'label': 'VERSION_STRING',
        'case_insensitive': False,
        'context_keywords': None,
    },
]

ENTROPY_CONTEXT_KEYWORDS = {
    'blob',
    'build',
    'checksum',
    'commit',
    'digest',
    'docker',
    'etag',
    'guid',
    'hash',
    'id',
    'image',
    'layer',
    'oid',
    'ref',
    'revision',
    'sha',
    'tree',
    'uuid',
    'version',
}

# ============================================================================
# Filter Logic
# ============================================================================

VALID_FILTERS = {'values', 'patterns', 'entropy', 'all'}


def is_env_disabled(name: str) -> bool:
    """Check if an environment variable is set to a falsy value."""
    val = os.environ.get(name, '')
    return val.lower() in ('0', 'false', 'no')


def is_env_enabled(name: str) -> bool:
    """Check if an environment variable is set to a truthy value."""
    val = os.environ.get(name, '')
    return val.lower() in ('1', 'true', 'yes')


def parse_filter_arg(args: list[str]) -> tuple[set[str], bool]:
    """Parse --filter/-f argument from command line.

    Returns:
        (set of filter names, whether --filter was present)
    """
    filters = set()
    found = False

    i = 0
    while i < len(args):
        arg = args[i]
        value = None

        if arg.startswith('--filter='):
            value = arg[len('--filter='):]
            found = True
        elif arg == '--filter' or arg == '-f':
            if i + 1 < len(args):
                value = args[i + 1]
                i += 1
            found = True

        if value is not None:
            # Parse comma-separated, case-insensitive, whitespace-trimmed
            for part in value.split(','):
                f = part.strip().lower()
                if f:
                    filters.add(f)

        i += 1

    return filters, found


def get_enabled_filters() -> tuple[bool, bool, bool]:
    """Determine which filters are enabled based on CLI and ENV.

    Returns:
        (values_enabled, patterns_enabled, entropy_enabled)
    """
    cli_filters, cli_present = parse_filter_arg(sys.argv[1:])

    if cli_present:
        # CLI overrides ENV entirely
        valid = set()
        invalid = []

        for f in cli_filters:
            if f in VALID_FILTERS:
                valid.add(f)
            else:
                invalid.append(f)

        # Warn about invalid filters
        for inv in invalid:
            print(f"secrets-filter: unknown filter '{inv}', ignoring", file=sys.stderr)

        # Error if no valid filters
        if not valid:
            print("secrets-filter: no valid filters specified", file=sys.stderr)
            sys.exit(1)

        # Expand 'all' to all filters (values + patterns + entropy)
        if 'all' in valid:
            return True, True, True

        return 'values' in valid, 'patterns' in valid, 'entropy' in valid

    # No CLI: use environment variables
    # values and patterns are enabled by default, entropy is disabled by default
    values_enabled = not is_env_disabled('SECRETS_FILTER_VALUES')
    patterns_enabled = not is_env_disabled('SECRETS_FILTER_PATTERNS')
    entropy_enabled = ENTROPY_ENABLED_DEFAULT or is_env_enabled('SECRETS_FILTER_ENTROPY')

    return values_enabled, patterns_enabled, entropy_enabled


# State machine states
STATE_NORMAL = 0
STATE_IN_PRIVATE_KEY = 1
STATE_IN_PRIVATE_KEY_OVERFLOW = 2

# Compile private key markers from imported strings
_PRIVATE_KEY_BEGIN_RE = re.compile(PRIVATE_KEY_BEGIN)
_PRIVATE_KEY_END_RE = re.compile(PRIVATE_KEY_END)

# Precompile all patterns at module level (avoid per-line compilation overhead)
# Format: (compiled_regex, label)
_COMPILED_PATTERNS: list[tuple[re.Pattern, str]] = [
    (re.compile(pattern), label) for pattern, label in PATTERNS
]
_COMPILED_CONTEXT_PATTERNS: list[tuple[re.Pattern, str]] = [
    (re.compile(pattern), label) for pattern, label in CONTEXT_PATTERNS
]

# Precompile special patterns
_COMPILED_SPECIAL_PATTERNS: dict[str, dict] = {
    name: {
        'regex': re.compile(spec['pattern']),
        'label': spec['label'],
        'secret_group': spec['secret_group'],
    }
    for name, spec in SPECIAL_PATTERNS.items()
}


def classify_segment(s: str) -> str:
    """Classify a segment: N=digits, A=letters, X=mixed."""
    if not s:
        return ''
    if s.isdigit():
        return f'{len(s)}N'
    if s.isalpha():
        return f'{len(s)}A'
    return f'{len(s)}X'


def describe_structure(s: str) -> str:
    """Describe token structure for redaction label.

    Examples:
        xoxb-123456789-987654321-abcdef -> xoxb-9N-9N-6X
        ghp_abc123def456... -> ghp_36X
        very_long_token... -> 108chars
    """
    # Very long tokens: just show length
    if len(s) >= LONG_THRESHOLD:
        # But still show prefix if there's a clear one
        for sep in ['-', '_', '.']:
            if sep in s:
                parts = s.split(sep)
                if parts[0].isalpha() or parts[0] in ('ghp', 'gho', 'ghs', 'ghr', 'npm', 'sk'):
                    return f'{parts[0]}{sep}...:{len(s)}chars'
        return f'{len(s)}chars'

    # Check for structured tokens (dash, underscore, or dot separated)
    for sep in ['-', '.', '_']:
        if sep in s:
            parts = s.split(sep)
            if len(parts) >= 2:
                # First part is likely a prefix if it's short alpha
                first = parts[0]
                if first.isalpha() and len(first) <= 12:
                    # Structured: prefix + segments
                    segments = [classify_segment(p) for p in parts[1:]]
                    return f'{first}{sep}' + sep.join(segments)
                # Otherwise describe all segments
                segments = [classify_segment(p) for p in parts]
                return sep.join(segments)

    # Simple token: just length + type
    return classify_segment(s)


def load_secrets() -> dict[str, str]:
    """Load secrets from environment variables.

    Checks explicit list of known secret variable names and pattern-matches
    variable names ending with common secret suffixes.
    Skips empty values and values shorter than 8 characters.
    """
    secrets = {}

    for name, value in os.environ.items():
        if not value or len(value) < 8:
            continue
        if name in EXPLICIT_ENV_VARS or any(name.endswith(p) for p in ENV_SUFFIXES):
            secrets[name] = value

    return secrets


def build_env_redactor(secrets: dict[str, str]) -> tuple[re.Pattern | None, dict[str, tuple[str, str]]]:
    """Build a combined regex pattern for all secret values.

    Returns:
        (compiled_pattern, lookup_dict) where lookup_dict maps escaped_value -> (var_name, structure)
        Returns (None, {}) if no secrets to match.
    """
    if not secrets:
        return None, {}

    # Sort by value length descending (longest first) to handle overlaps
    sorted_secrets = sorted(secrets.items(), key=lambda x: -len(x[1]))

    # Build lookup: escaped_value -> (var_name, structure)
    lookup: dict[str, tuple[str, str]] = {}
    escaped_values = []

    for var, val in sorted_secrets:
        if not val:
            continue
        escaped = re.escape(val)
        escaped_values.append(escaped)
        # Store with the original value as key for the replacement function
        lookup[val] = (var, describe_structure(val))

    if not escaped_values:
        return None, {}

    # Build combined pattern: (val1|val2|val3|...)
    # Using non-capturing group for efficiency
    combined = '(?:' + '|'.join(escaped_values) + ')'
    return re.compile(combined), lookup


def redact_env_values(text: str, secrets: dict[str, str],
                      env_regex: re.Pattern | None = None,
                      env_lookup: dict[str, tuple[str, str]] | None = None) -> str:
    """Replace known secret values with [REDACTED:VAR_NAME:structure], longest first.

    If env_regex and env_lookup are provided, uses single-pass regex replacement.
    Otherwise falls back to iterative replacement (for backwards compatibility).
    """
    if env_regex is not None and env_lookup is not None:
        # Single-pass replacement using combined regex
        def replace_match(m):
            matched = m.group(0)
            var, structure = env_lookup[matched]
            return f'[REDACTED:{var}:{structure}]'
        return env_regex.sub(replace_match, text)

    # Fallback: iterative replacement (for backwards compatibility)
    for var, val in sorted(secrets.items(), key=lambda x: -len(x[1])):
        if val:
            structure = describe_structure(val)
            text = text.replace(val, f'[REDACTED:{var}:{structure}]')
    return text


def redact_patterns(text: str) -> str:
    """Replace known token patterns with [REDACTED:LABEL:structure]."""
    # Apply direct patterns (precompiled)
    for regex, label in _COMPILED_PATTERNS:
        def make_replacement(m, lbl=label):
            matched = m.group(0)
            structure = describe_structure(matched)
            return f'[REDACTED:{lbl}:{structure}]'
        text = regex.sub(make_replacement, text)

    # Apply context patterns (precompiled)
    for regex, label in _COMPILED_CONTEXT_PATTERNS:
        def make_replacement(m, lbl=label):
            matched = m.group(0)
            structure = describe_structure(matched)
            return f'[REDACTED:{lbl}:{structure}]'
        text = regex.sub(make_replacement, text)

    # Special patterns requiring capture groups (precompiled)
    for name, spec in _COMPILED_SPECIAL_PATTERNS.items():
        regex = spec['regex']
        label = spec['label']
        secret_group = spec['secret_group']

        def make_special_replacement(m, lbl=label, grp=secret_group):
            structure = describe_structure(m.group(grp))
            # Reconstruct with all groups, replacing secret group
            parts = []
            for i in range(1, len(m.groups()) + 1):
                if i == grp:
                    parts.append(f'[REDACTED:{lbl}:{structure}]')
                else:
                    parts.append(m.group(i))
            return ''.join(parts)

        text = regex.sub(make_special_replacement, text)

    return text


# ============================================================================
# Entropy-based detection
# ============================================================================

# Character sets for classification
_CHARSET_HEX = frozenset('0123456789abcdef')
_CHARSET_BASE64 = frozenset('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=')
_CHARSET_ALNUM = frozenset('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789_-')

# Precompile exclusion patterns
_ENTROPY_EXCLUSION_PATTERNS = []
for excl in ENTROPY_EXCLUSIONS:
    flags = re.IGNORECASE if excl.get('case_insensitive') else 0
    _ENTROPY_EXCLUSION_PATTERNS.append({
        'regex': re.compile(excl['pattern'], flags),
        'label': excl['label'],
        'context_keywords': excl.get('context_keywords'),
    })

# Token extraction regex: split on delimiters
# Includes = for key=value pairs, @ for URLs, # for anchors
_TOKEN_DELIM_RE = re.compile(r'[\s"\'`()\[\]{},:;<>=@#]+')


def shannon_entropy(s: str) -> float:
    """Calculate Shannon entropy of a string in bits.

    H = -Σ p(x) log₂ p(x)
    """
    if not s:
        return 0.0
    counts = Counter(s)
    length = len(s)
    entropy = 0.0
    for count in counts.values():
        p = count / length
        entropy -= p * math.log2(p)
    return entropy


def classify_charset(s: str) -> str:
    """Classify a string's character set.

    Returns: 'hex', 'base64', 'alphanumeric', or 'mixed'
    """
    chars = set(s.lower())

    # Check hex first (most restrictive)
    if chars <= _CHARSET_HEX:
        return 'hex'

    # Check alphanumeric (common for tokens)
    upper_chars = set(s)
    if upper_chars <= _CHARSET_ALNUM:
        return 'alphanumeric'

    # Check base64
    if upper_chars <= _CHARSET_BASE64:
        return 'base64'

    return 'mixed'


def extract_tokens(text: str, min_len: int, max_len: int) -> list[tuple[str, int, int]]:
    """Extract potential secret tokens from text.

    Returns: list of (token, start_pos, end_pos)
    """
    tokens = []
    # Split by delimiters while tracking positions
    pos = 0
    for part in _TOKEN_DELIM_RE.split(text):
        if part:
            start = text.find(part, pos)
            end = start + len(part)
            pos = end

            # Filter by length
            if not (min_len <= len(part) <= max_len):
                continue

            # Skip if all alphabetic (variable names)
            if part.isalpha():
                continue

            # Skip if all numeric (IDs, line numbers)
            if part.isdigit():
                continue

            # Skip if contains whitespace (shouldn't after split, but be safe)
            if any(c in part for c in ' \t\n'):
                continue

            tokens.append((part, start, end))

    return tokens


def has_context_keyword(text: str, pos: int, keywords: list[str] | None) -> bool:
    """Check if a position in text is preceded by a context keyword.

    Looks back up to 50 characters for any of the keywords.
    """
    if not keywords:
        return False

    # Look back up to 50 chars
    start = max(0, pos - 50)
    prefix = text[start:pos].lower()

    for kw in keywords:
        if kw.lower() in prefix:
            return True

    return False


def matches_exclusion(token: str, text: str, pos: int) -> str | None:
    """Check if token matches an exclusion pattern.

    Returns: label if excluded, None otherwise
    """
    for excl in _ENTROPY_EXCLUSION_PATTERNS:
        regex = excl['regex']
        if regex.fullmatch(token):
            # Check context keywords if present
            context_kw = excl['context_keywords']
            if context_kw:
                if has_context_keyword(text, pos, context_kw):
                    return excl['label']
                # Has context keywords but none found - not excluded
                continue
            # No context keywords required - excluded
            return excl['label']

    # Check global context keywords
    if has_context_keyword(text, pos, list(ENTROPY_CONTEXT_KEYWORDS)):
        return 'CONTEXT'

    return None


def describe_entropy_structure(token: str, entropy: float, charset: str) -> str:
    """Create structure description for entropy redaction.

    Example: hex:40:3.8
    """
    charset_abbrev = {
        'hex': 'hex',
        'base64': 'b64',
        'alphanumeric': 'alnum',
        'mixed': 'mix',
    }.get(charset, charset)
    return f'{charset_abbrev}:{len(token)}:{entropy:.1f}'


def get_entropy_config() -> dict:
    """Get entropy configuration from environment overrides or defaults."""
    config = {
        'thresholds': dict(ENTROPY_THRESHOLDS),
        'min_length': ENTROPY_MIN_LENGTH,
        'max_length': ENTROPY_MAX_LENGTH,
    }

    # Check for global threshold override
    global_threshold = os.environ.get('SECRETS_FILTER_ENTROPY_THRESHOLD')
    if global_threshold:
        try:
            t = float(global_threshold)
            config['thresholds'] = {'hex': t, 'base64': t, 'alphanumeric': t}
        except ValueError:
            pass

    # Check for per-charset overrides
    for charset in ['hex', 'base64']:
        env_name = f'SECRETS_FILTER_ENTROPY_{charset.upper()}'
        val = os.environ.get(env_name)
        if val:
            try:
                config['thresholds'][charset] = float(val)
            except ValueError:
                pass

    # Length overrides
    min_len = os.environ.get('SECRETS_FILTER_ENTROPY_MIN_LEN')
    if min_len:
        try:
            config['min_length'] = int(min_len)
        except ValueError:
            pass

    max_len = os.environ.get('SECRETS_FILTER_ENTROPY_MAX_LEN')
    if max_len:
        try:
            config['max_length'] = int(max_len)
        except ValueError:
            pass

    return config


def redact_entropy(text: str, config: dict | None = None) -> str:
    """Detect and redact high-entropy strings.

    Processes the text, finds tokens with high entropy, and replaces them
    if they don't match exclusion patterns (like git SHAs, UUIDs, etc.)
    """
    if config is None:
        config = get_entropy_config()

    min_len = config['min_length']
    max_len = config['max_length']
    thresholds = config['thresholds']

    tokens = extract_tokens(text, min_len, max_len)

    # Process in reverse order to preserve positions when replacing
    replacements = []
    for token, start, end in reversed(tokens):
        # Check exclusions
        excluded = matches_exclusion(token, text, start)
        if excluded:
            continue

        # Classify character set and get threshold
        charset = classify_charset(token)
        if charset == 'mixed':
            # Mixed character sets are harder to classify - use alphanumeric threshold
            threshold = thresholds.get('alphanumeric', 4.5)
        else:
            threshold = thresholds.get(charset, 4.5)

        # Calculate entropy
        entropy = shannon_entropy(token)

        if entropy >= threshold:
            structure = describe_entropy_structure(token, entropy, charset)
            replacement = f'[REDACTED:HIGH_ENTROPY:{structure}]'
            replacements.append((start, end, replacement))

    # Apply replacements in reverse order
    for start, end, replacement in replacements:
        text = text[:start] + replacement + text[end:]

    return text


def redact_line(line: str, secrets: dict[str, str] | None,
                values_enabled: bool, patterns_enabled: bool,
                entropy_enabled: bool = False, entropy_config: dict | None = None,
                env_regex: re.Pattern | None = None,
                env_lookup: dict[str, tuple[str, str]] | None = None) -> str:
    """Redact a single line using env values, patterns, and/or entropy."""
    if values_enabled and secrets:
        line = redact_env_values(line, secrets, env_regex, env_lookup)
    if patterns_enabled:
        line = redact_patterns(line)
    if entropy_enabled:
        line = redact_entropy(line, entropy_config)
    return line


def flush_buffer_redacted(buffer: list[str], secrets: dict[str, str] | None,
                          values_enabled: bool, patterns_enabled: bool,
                          entropy_enabled: bool = False,
                          entropy_config: dict | None = None,
                          env_regex: re.Pattern | None = None,
                          env_lookup: dict[str, tuple[str, str]] | None = None) -> None:
    """Flush buffered lines, redacting each individually."""
    for line in buffer:
        sys.stdout.write(redact_line(line, secrets, values_enabled, patterns_enabled,
                                     entropy_enabled, entropy_config, env_regex, env_lookup))
        sys.stdout.flush()


def main():
    """Stream stdin line-by-line with state machine for private keys."""
    # Determine which filters are enabled
    values_enabled, patterns_enabled, entropy_enabled = get_enabled_filters()

    # Load secrets only if values filter is enabled
    secrets = load_secrets() if values_enabled else None

    # Build combined env regex once at startup (avoids per-line regex compilation)
    env_regex, env_lookup = build_env_redactor(secrets) if secrets else (None, None)

    # Load entropy config only if entropy filter is enabled
    entropy_config = get_entropy_config() if entropy_enabled else None

    state = STATE_NORMAL
    buffer: list[str] = []

    # Read raw bytes to handle binary data correctly (before UTF-8 conversion)
    stdin_buffer = sys.stdin.buffer
    stdout_buffer = sys.stdout.buffer

    while True:
        line_bytes = stdin_buffer.readline()
        if not line_bytes:
            break  # EOF

        # Binary detection: null byte (check raw bytes before UTF-8 conversion)
        if b'\x00' in line_bytes:
            # Flush any buffered content
            if buffer:
                flush_buffer_redacted(buffer, secrets, values_enabled, patterns_enabled,
                                     entropy_enabled, entropy_config, env_regex, env_lookup)
                buffer = []
            # Passthrough this line and rest of input unchanged as raw bytes
            stdout_buffer.write(line_bytes)
            stdout_buffer.flush()
            # Copy rest of stdin to stdout
            while True:
                rest = stdin_buffer.read(8192)
                if not rest:
                    break
                stdout_buffer.write(rest)
            return

        # Decode to string (lossy for invalid UTF-8 - rare edge case)
        line = line_bytes.decode('utf-8', errors='replace')

        if state == STATE_NORMAL:
            if patterns_enabled and _PRIVATE_KEY_BEGIN_RE.search(line):
                # Start buffering private key block (only if patterns enabled)
                state = STATE_IN_PRIVATE_KEY
                buffer = [line]
            else:
                # Normal line: redact and output immediately
                sys.stdout.write(redact_line(line, secrets, values_enabled, patterns_enabled,
                                             entropy_enabled, entropy_config, env_regex, env_lookup))
                sys.stdout.flush()

        elif state == STATE_IN_PRIVATE_KEY:
            buffer.append(line)

            if _PRIVATE_KEY_END_RE.search(line):
                # Complete private key block - emit single redaction
                sys.stdout.write('[REDACTED:PRIVATE_KEY:multiline]\n')
                sys.stdout.flush()
                buffer = []
                state = STATE_NORMAL

            elif len(buffer) > MAX_PRIVATE_KEY_BUFFER:
                # Buffer overflow - redact entirely (fail closed, don't leak)
                sys.stdout.write('[REDACTED:PRIVATE_KEY:multiline]\n')
                sys.stdout.flush()
                buffer = []
                # Transition to overflow state - consume remaining lines silently until END
                state = STATE_IN_PRIVATE_KEY_OVERFLOW

        elif state == STATE_IN_PRIVATE_KEY_OVERFLOW:
            # Consume lines silently until END marker
            if _PRIVATE_KEY_END_RE.search(line):
                state = STATE_NORMAL
            # No buffering, no output - just wait for END

    # EOF: handle remaining state
    if state == STATE_IN_PRIVATE_KEY:
        # Incomplete private key block - redact entirely (fail closed, don't leak)
        sys.stdout.write('[REDACTED:PRIVATE_KEY:multiline]\n')
        sys.stdout.flush()
    elif state == STATE_IN_PRIVATE_KEY_OVERFLOW:
        # Already emitted overflow redaction, nothing to do
        pass
    elif buffer:
        # Flush any remaining buffered content
        flush_buffer_redacted(buffer, secrets, values_enabled, patterns_enabled,
                             entropy_enabled, entropy_config, env_regex, env_lookup)


if __name__ == '__main__':
    main()